---
title: "Machine Learning Assignment"
author: "Dr. Peter Bradley"
date: "April 6, 2016"
output: html_document
---
```{r echo=FALSE}
library(caret)
```

## Loading data

```{r}
training <- read.csv("pml-training.csv", stringsAsFactors = TRUE, na.strings=c("", "NA", "#DIV/0"))
testing <- read.csv("pml-testing.csv", stringsAsFactors = TRUE, na.strings=c("", "NA", "#DIV/0"))
```
Using 'wearable' motion tracking devices, Velloso, E. et. al (2013) collected a data set of `r nrow(training) + nrow(testing)` observations of 6 different participants performing barbell lifts in 5 different ways.  The 'correct' way was identified as 'A', with 'B', 'C', 'D' and 'E' identifying common mistakes of beginners. The goal of this analysis is to determine if it is possible to predict what method the participant is using based only on the output of the wearables.  Such a prediction could automate fitness coaching and reduce injuries associated with misuse of exercise equipment.

## Getting and cleaning data
First, the data comes to us already separated into 'training' and 'testing' sets. The training set is loaded, and the testing set aside until the model is complete.

This data is not pristene---some of the columns contain empty strings, which need to be idenfied as 'NA'. Second, there are some variables named 'kurtosis' which are processed as strings, because they contain "#DIV/0". We'll fix both of these problems at import using 'na.strings'. Since we've removed all the string variables except the 'classe' variable, which is our target, we can make classe a factor at import with 'stringsAsFactors = TRUE'.

The training table has a large number of rows with NA's or blank values - `r nrow(training[is.na(training$min_roll_belt),])` rows to be exact. We do not want to remove these rows, as that would greatly hamper the development of our model. 

Instead, we want to ignore the variables for which these `r nrow(training[is.na(training$min_roll_belt),])` rows are blank.

```{r}
findNull <- function(y) { sum(length(which(is.na(y) ))) }
na_count <- do.call(rbind, lapply(training, findNull))
na_count <- as.data.frame(na_count)
na_count$name <- rownames(na_count)
#na_count[na_count$V1 > 0,]$name
which(na_count$V1 > 0) -> varsToIgnore
```

Now we remove the record keeping variables 'X', 'username', *timestamp*, 'new_window' and 'num_window'.

```{r}
set.seed(424242)
varsToIgnore <- c(1,2,3,4,5,6,7,varsToIgnore)
#summary(training[, -varsToIgnore])
training[, -varsToIgnore] -> newtrainingset

```

## Building the Model
```{r}
training[, -varsToIgnore] -> newtrainingset
subTrain <- createDataPartition(y=newtrainingset$classe, p=0.7, list=FALSE)
subTrain1 <- newtrainingset[subTrain,]
subTrain2 <- newtrainingset[-subTrain,]
```
Before building the model, we split the training set into two, to allow cross-validation without introducing the testing set to the process of building the model. `r nrow(subTrain1)` with `r ncol(subTrain1)` variables were used for training, `r nrow(subTrain2)` with `r ncol(subTrain2)` variables were used for cross-validation.. 

Models initially tested against all variables included Trees, Random Forest, Linear Disciminant Analysis and Naive Bayes.[^processing]
[^processing]:Due to the constraints of my system, only the final model discussed herein is actually run in the compilation of this document. All values are from previous runs. The R code is left in, but commented out, so the interested reader could, if he or she wanted to verify, run the code on a faster machine.
```{r}
#modFitTrees <- train(classe ~  ., data=subTrain1, method="rpart")
modFitRF1 <- train(classe ~  ., data=subTrain1, method="rf")
#modFitLDA <- train(classe ~  ., data=subTrain1, method="lda")
#modFitNB <- train(classe ~  ., data=subTrain1, method="nb")
```

The accuracy of the resultant models except for Random Forest left something to be desired:
```{r}
#modFitTrees$results['Accuracy'][1,]
modFitRF1$results['Accuracy'][1,]
#modFitLDA$results['Accuracy'][1,]
#modFitNB$results['Accuracy'][1,]
```
Trees: 50.32%
RF1: `r modFitRF1$results['Accuracy'][1,]`
LDA: 70.35%
NB: 51.34%
````{r}
#modFitGBM1 <- train(classe ~  ., data=subTrain1, method="gbm", verbose=FALSE)
```

To improve the model, we tried Generalized Boosted Models, or 'gbm, which produced an initially exiciting accuracy of 75.00%

### Selection of relevant variables
```{r}
varsToInclude <- c("roll_belt", "pitch_forearm", "yaw_belt", "magnet_dumbbell_y", "magnet_dumbbell_z", "roll_forearm", "magnet_belt_z", "gyros_belt_z", "pitch_belt", "roll_dumbbell","classe")

varsToExtend <- c("accel_forearm_x", "accel_dumbbell_y", "gyros_dumbbell_y", "magnet_arm_z", "magnet_forearm_z", "accel_dumbbell_x", "yaw_arm", "magnet_dumbbell_x", "accel_forearm_z", "magnet_belt_y")
```

By reviewing the importance of the variables in the output of the Random Forest model, we reduced the list of variables to `r c(varsToInclude, varsToExtend)`, and `r varsToInclude`.  

```{r}
modFitRF2 <- train(classe ~  roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_y + magnet_dumbbell_z + roll_forearm + magnet_belt_z + gyros_belt_z + pitch_belt + roll_dumbbell  + accel_forearm_x + accel_dumbbell_y + gyros_dumbbell_y + magnet_arm_z + magnet_forearm_z + accel_dumbbell_x + yaw_arm + magnet_dumbbell_x + accel_forearm_z + magnet_belt_y, data=subTrain1, method="rf")
#modFitRF3 <- train(classe ~  roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_y + magnet_dumbbell_z + roll_forearm + magnet_belt_z + gyros_belt_z + pitch_belt + roll_dumbbell, data=subTrain1, method="rf")
#modFitGBM2 <- train(classe ~  roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_y + magnet_dumbbell_z + roll_forearm + magnet_belt_z + gyros_belt_z + pitch_belt + roll_dumbbell  + accel_forearm_x + accel_dumbbell_y + gyros_dumbbell_y + magnet_arm_z + magnet_forearm_z + accel_dumbbell_x + yaw_arm + magnet_dumbbell_x + accel_forearm_z + magnet_belt_y, data=subTrain1, method="gbm", verbose=FALSE)
#modFitGBM3 <- train(classe ~  roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_y + magnet_dumbbell_z + roll_forearm + magnet_belt_z + gyros_belt_z + pitch_belt + roll_dumbbell, data=subTrain1, method="gbm", verbose=FALSE)
```

Neither restriction improved either model, with a decrease in accuracy of the RF model to 98.71 and 98.20 respectively, and 74.15 and 71.68 for the GBM model.

### Cross validation
We also attempted to tune the models by introducing cross validation into the training routine itself using the 'trControl' argument to 'train'.  Leave-one-out and repeated K-fold proved too computational intensive for practical use, so we restricted our experimentation to Bootstrap (method="boot") and Cross-Validation (method="cv").  

```{r}
#train_control <- trainControl(method="boot", number=100)
train_control <- trainControl(method="cv", number=10)
# train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
# repeatedcv is too intensive. 
# train_control <- trainControl(method="LOOCV")
# Leave one out is too intensive. 
#modFitRF4 <- train(classe ~  roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_y + magnet_dumbbell_z + roll_forearm + magnet_belt_z + gyros_belt_z + pitch_belt + roll_dumbbell  + accel_forearm_x + accel_dumbbell_y + gyros_dumbbell_y + magnet_arm_z + magnet_forearm_z + accel_dumbbell_x + yaw_arm + magnet_dumbbell_x + accel_forearm_z + magnet_belt_y, data=subTrain1, trControl=trainControl(method="cv"), method="rf")
#modFitGBM4 <- train(classe ~  roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_y + magnet_dumbbell_z + roll_forearm + magnet_belt_z + gyros_belt_z + pitch_belt + roll_dumbbell, data=subTrain1, method="gbm", trControl=train_control, verbose=FALSE)
```

The accuracy of RF model increased to 99.12% for RF but decreased to 71.62% for GBM.

The initial, unmodified GBM and RF models yeild the promising results, with an accuracy of `r modFitRF1$results['Accuracy'][1,]` and 75.00% for the full set of variables.

To cross-validate this finding, we run each model generated on the reserved set of data, and calculate the accuracy.
```{r}
#trainControl 

#subTrain2$predicttrees <- predict(modFitTrees, newdata=subTrain2)
#subTrain2$predictrf1 <- predict(modFitRF1, newdata=subTrain2)
subTrain2$predictrf2 <- predict(modFitRF2, newdata=subTrain2)
#subTrain2$predictrf3 <- predict(modFitRF3, newdata=subTrain2)
#subTrain2$predictrf4 <- predict(modFitRF4, newdata=subTrain2)
#subTrain2$predictgbm1 <- predict(modFitGBM1, newdata=subTrain2)
#subTrain2$predictgbm2 <- predict(modFitGBM2, newdata=subTrain2)
#subTrain2$predictgbm3 <- predict(modFitGBM3, newdata=subTrain2)
#subTrain2$predictgbm4 <- predict(modFitGBM4, newdata=subTrain2)

#subTrain2$predictlda <- predict(modFitLDA, newdata=subTrain2)
#subTrain2$predictnb <- predict(modFitNB, newdata=subTrain2)


```

## Predictions
To test the accuracy of the model, we find the percentage of accurate predictions against the cross-validation set.  These are:
```{r}
#accuracyTrees <- round(sum(subTrain2$predicttrees == subTrain2$classe)/length(subTrain2$classe) * 100,2)
accuracyrf1 <- round(sum(subTrain2$predictrf1 == subTrain2$classe)/length(subTrain2$classe) * 100,2)
accuracyrf2 <- round(sum(subTrain2$predictrf2 == subTrain2$classe)/length(subTrain2$classe) * 100,2)
#accuracyrf3 <- round(sum(subTrain2$predictrf3 == subTrain2$classe)/length(subTrain2$classe) * 100,2)
#accuracyrf4 <- round(sum(subTrain2$predictrf4 == subTrain2$classe)/length(subTrain2$classe) * 100,2)
#accuracygbm1 <- round(sum(subTrain2$predictgbm1 == subTrain2$classe)/length(subTrain2$classe) * 100,2)
#accuracygbm2 <- round(sum(subTrain2$predictgbm2 == subTrain2$classe)/length(subTrain2$classe) * 100,2)
#accuracygbm3 <- round(sum(subTrain2$predictgbm3 == subTrain2$classe)/length(subTrain2$classe) * 100,2)
#accuracygbm4 <- round(sum(subTrain2$predictgbm4 == subTrain2$classe)/length(subTrain2$classe) * 100,2)
```
Accuracy RF1: `r accuracyrf1`%
Accuracy RF2: `r accuracyrf2`%
Accuracy RF3: 98.5%
Accuracy RF4: 99.35%
Accuracy GBM1: 96.52%
Accuracy GBM2: 95.8%
Accuracy GBM3: 93.39%
Accuracy GBM4: 93.63%

Accuracy calcuations are consistent with the accuracy reported by 'train', meaning that the un-restricted RF1 and GBM1 are the most accurate models tested. We'd expect a `r 100-accuracyrf2`% error rate for the RF2 model, and a 3.58% error rate for the GBM1 model.



Confusion matrices for the relevant models show the strengths of each:

```{r}
confusionMatrix(subTrain2$predictrf2, subTrain2$classe)
#confusionMatrix(subTrain2$predictgbm1, subTrain2$classe)
```

### Combining models
As a final attempt to increase the accuracy of the model, we try combining the predictions from the successful RF2 and GBM1 models and retrain it using 'gam' method. 
```{r}
#newmodel <- train(classe ~ predictrf2 + predictgbm1 , method="gam", data=subTrain2)
#subTrain2$predictcombined <- predict(newmodel, newdata=subTrain2)
#accuracycombined <- round(sum(subTrain2$predictcombined == subTrain2$classe)/length(subTrain2$classe) * 100,2)
#confusionMatrix(subTrain2$predictcombined, subTrain2$classe) -> newmodelout
#only produces accuracy of 46.4
```

The resultant model is only accurate to 0.4766. (`r #newmodelout$overall['Accuracy']`), experimentally verified through prediction on the cross-validation set to 47.66% (`r #accuracycombined`).

## Conclusion

Thus, the best model on this initial training set is simple random forest model, with a restricted variables included. The predictions versus actual of the cross-validated set are displayed below:

```{r}
#finalMod <- train(classe ~  roll_belt + pitch_forearm + yaw_belt + magnet_dumbbell_y + magnet_dumbbell_z + roll_forearm + magnet_belt_z + gyros_belt_z + pitch_belt + roll_dumbbell  + accel_forearm_x + accel_dumbbell_y + gyros_dumbbell_y + magnet_arm_z + magnet_forearm_z + accel_dumbbell_x + yaw_arm + magnet_dumbbell_x + accel_forearm_z + magnet_belt_y, method="rf", data=newtrainingset)
qplot(subTrain2$classe, subTrain2$predictrf2) + geom_jitter() + xlab("Original") + ylab("Prediction")
```

## Citations
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz453NWlNvG